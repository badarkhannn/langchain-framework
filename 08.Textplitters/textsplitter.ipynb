{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fba75c",
   "metadata": {},
   "source": [
    "### Length based Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b05d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: This is a long text that needs to be split into sm\n",
      "----------------------------------------\n",
      "Chunk 2: it into smaller chunks. The text contains multiple\n",
      "----------------------------------------\n",
      "Chunk 3: s multiple sentences and paragraphs, which makes i\n",
      "----------------------------------------\n",
      "Chunk 4: ch makes it suitable for testing the text splitter\n",
      "----------------------------------------\n",
      "Chunk 5: t splitter functionality. The goal is to ensure th\n",
      "----------------------------------------\n",
      "Chunk 6: ensure that the text is divided into manageable p\n",
      "----------------------------------------\n",
      "Chunk 7: nageable pieces without losing the context or mean\n",
      "----------------------------------------\n",
      "Chunk 8: xt or meaning of the original content.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"This is a long text that needs to be split into smaller chunks. The text contains multiple sentences and paragraphs, which makes it suitable for testing the text splitter functionality. The goal is to ensure that the text is divided into manageable pieces without losing the context or meaning of the original content.\"\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50,chunk_overlap=10, separator=\"\")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "    print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c037cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_Loader import pyPdfLoader\n",
    "\n",
    "loder = pyPdfLoader.PyPDFLoader(\"example.pdf\")\n",
    "\n",
    "docs = loder.load()\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50,chunk_overlap=10, separator=\"\")\n",
    "\n",
    "chunks = text_splitter.split_document(text)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919fc72d",
   "metadata": {},
   "source": [
    "### Text-Strcuture basex Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e13e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: This is a long text\n",
      "----------------------------------------\n",
      "Chunk 2: that needs to be\n",
      "----------------------------------------\n",
      "Chunk 3: split into smaller\n",
      "----------------------------------------\n",
      "Chunk 4: chunks. The text\n",
      "----------------------------------------\n",
      "Chunk 5: contains multiple\n",
      "----------------------------------------\n",
      "Chunk 6: sentences and\n",
      "----------------------------------------\n",
      "Chunk 7: paragraphs, which\n",
      "----------------------------------------\n",
      "Chunk 8: makes it suitable\n",
      "----------------------------------------\n",
      "Chunk 9: for testing the\n",
      "----------------------------------------\n",
      "Chunk 10: text splitter\n",
      "----------------------------------------\n",
      "Chunk 11: functionality. The\n",
      "----------------------------------------\n",
      "Chunk 12: goal is to ensure\n",
      "----------------------------------------\n",
      "Chunk 13: that the text is\n",
      "----------------------------------------\n",
      "Chunk 14: divided into\n",
      "----------------------------------------\n",
      "Chunk 15: manageable pieces\n",
      "----------------------------------------\n",
      "Chunk 16: without losing the\n",
      "----------------------------------------\n",
      "Chunk 17: context or meaning\n",
      "----------------------------------------\n",
      "Chunk 18: of the original\n",
      "----------------------------------------\n",
      "Chunk 19: content.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"This is a long text that needs to be split into smaller chunks. The text contains multiple sentences and paragraphs, which makes it suitable for testing the text splitter functionality. The goal is to ensure that the text is divided into manageable pieces without losing the context or meaning of the original content.\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=1,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9639c",
   "metadata": {},
   "source": [
    "### For specific text-structured like code etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "270f407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: class Animal:\n",
      "----------------------------------------\n",
      "Chunk 2: def\n",
      "----------------------------------------\n",
      "Chunk 3: __init__(self,\n",
      "----------------------------------------\n",
      "Chunk 4: name):\n",
      "----------------------------------------\n",
      "Chunk 5: self.name =\n",
      "----------------------------------------\n",
      "Chunk 6: name\n",
      "----------------------------------------\n",
      "Chunk 7: def\n",
      "----------------------------------------\n",
      "Chunk 8: speak(self):\n",
      "----------------------------------------\n",
      "Chunk 9: raise\n",
      "----------------------------------------\n",
      "Chunk 10: NotImplementedError\n",
      "----------------------------------------\n",
      "Chunk 11: r(\"Subclass\n",
      "----------------------------------------\n",
      "Chunk 12: must implement\n",
      "----------------------------------------\n",
      "Chunk 13: abstract method\")\n",
      "----------------------------------------\n",
      "Chunk 14: class Dog(Animal):\n",
      "----------------------------------------\n",
      "Chunk 15: def\n",
      "----------------------------------------\n",
      "Chunk 16: speak(self):\n",
      "----------------------------------------\n",
      "Chunk 17: return\n",
      "----------------------------------------\n",
      "Chunk 18: f\"{self.name} says\n",
      "----------------------------------------\n",
      "Chunk 19: Woof!\"\n",
      "----------------------------------------\n",
      "Chunk 20: class Cat(Animal):\n",
      "----------------------------------------\n",
      "Chunk 21: def\n",
      "----------------------------------------\n",
      "Chunk 22: speak(self):\n",
      "----------------------------------------\n",
      "Chunk 23: return\n",
      "----------------------------------------\n",
      "Chunk 24: f\"{self.name} says\n",
      "----------------------------------------\n",
      "Chunk 25: Meow!\"\n",
      "----------------------------------------\n",
      "Chunk 26: animals =\n",
      "----------------------------------------\n",
      "Chunk 27: [Dog(\"Buddy\"),\n",
      "----------------------------------------\n",
      "Chunk 28: Cat(\"Whiskers\")]\n",
      "----------------------------------------\n",
      "Chunk 29: random_animal =\n",
      "----------------------------------------\n",
      "Chunk 30: random.choice(anima\n",
      "----------------------------------------\n",
      "Chunk 31: als)\n",
      "----------------------------------------\n",
      "Chunk 32: print(random_animal\n",
      "----------------------------------------\n",
      "Chunk 33: l.speak())\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "import random\n",
    "\n",
    "text = \"\"\" \n",
    "\n",
    "class Animal:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def speak(self):\n",
    "        raise NotImplementedError(\"Subclass must implement abstract method\")\n",
    "\n",
    "class Dog(Animal):\n",
    "    def speak(self):\n",
    "        return f\"{self.name} says Woof!\"\n",
    "\n",
    "class Cat(Animal):\n",
    "    def speak(self):\n",
    "        return f\"{self.name} says Meow!\"\n",
    "\n",
    "animals = [Dog(\"Buddy\"), Cat(\"Whiskers\")]\n",
    "random_animal = random.choice(animals)\n",
    "print(random_animal.speak())\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=1,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a5294d",
   "metadata": {},
   "source": [
    "### Semantic Meaning Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e78652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "text = \"This is a long text that needs to be split into smaller chunks. The text contains multiple sentences and paragraphs, which makes it suitable for testing the text splitter functionality. The goal is to ensure that the text is divided into manageable pieces without losing the context or meaning of the original content.\"\n",
    "\n",
    "##### This is the still not working perfect as compared to other so will update the code later if I got time "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
